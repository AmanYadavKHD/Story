üìÅ project_folder/
‚îÇ
‚îú‚îÄ‚îÄ Data_Ingestion/
‚îÇ   ‚îî‚îÄ‚îÄ final_cleaned_ticker_data.json
‚îÇ
‚îú‚îÄ‚îÄ main.py                  # To run everything
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ load_data.py         # Load data
‚îÇ   ‚îú‚îÄ‚îÄ filter_data.py       # Filtering function
‚îÇ   ‚îî‚îÄ‚îÄ validate_data.py     # Validation script


# utils/load_data.py
import json
import os

def load_transcript_data(file_path='Data_Ingestion/final_cleaned_ticker_data.json'):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


# utils/filter_data.py
def filter_transcripts(data, company=None, year=None, quarter=None, keyword=None):
    results = []
    for item in data:
        if company and company.lower() not in item['company'].lower():
            continue
        if year and int(item['year']) != int(year):
            continue
        if quarter and item['quarter'].upper() != quarter.upper():
            continue
        if keyword and keyword.lower() not in item['content'].lower():
            continue
        results.append(item)
    return results


# utils/validate_data.py
def validate_transcripts(data):
    required_fields = {'company', 'file_name', 'quarter', 'year', 'date', 'speaker', 'role', 'content'}
    errors = []
    for idx, item in enumerate(data):
        missing = required_fields - item.keys()
        if missing:
            errors.append(f"Item {idx} is missing fields: {missing}")
        elif not all(item[field] for field in required_fields):
            errors.append(f"Item {idx} has empty fields")
    return errors

# main.py
import json
from utils.load_data import load_transcript_data
from utils.filter_data import filter_transcripts
from utils.validate_data import validate_transcripts
from utils.chunk_data import prepare_chunks

def save_chunked_data(chunks, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ Chunked data saved to {file_path}")

# Step 1: Load
data = load_transcript_data()

# Step 2: Validate
errors = validate_transcripts(data)
if errors:
    print("‚ö†Ô∏è Data validation errors found:")
    for err in errors:
        print(err)
    exit(1)
else:
    print("‚úÖ Data is valid.\n")

# Step 3: Filter
results = filter_transcripts(data, company='Apple', year=2025, quarter='Q1', keyword='sales')

# Step 4: Chunking
chunks = prepare_chunks(data)
print(f"‚úÖ Prepared {len(chunks)} semantic chunks.")
print("Sample chunk:")
print(chunks[0])

# Step 4.1: Save chunked data
chunked_file_path = 'Data_Ingestion/final_cleaned_ticker_data_chunked.json'
save_chunked_data(chunks, chunked_file_path)

# Step 5: Show filtered results
for res in results:
    print(f"{res['date']} - {res['speaker']} ({res['role']}):")
    print(res['content'])
    print('-' * 80)


#chunking
import textwrap

def prepare_chunks(data, max_chunk_chars=1000, overlap=200):
    """
    Prepares semantic chunks with metadata.
    Long content is split into smaller overlapping chunks.

    Parameters:
    - data: list of transcript dicts (already per speaker)
    - max_chunk_chars: max characters per chunk (default: 1000)
    - overlap: number of overlapping chars between chunks (default: 200)

    Returns:
    - List of chunk dicts with metadata and short content
    """
    all_chunks = []

    for item in data:
        content = item.get("content", "")
        if not content.strip():
            continue

        # Break content into overlapping chunks
        chunks = []
        if len(content) <= max_chunk_chars:
            chunks = [content]
        else:
            start = 0
            while start < len(content):
                end = start + max_chunk_chars
                chunk = content[start:end]
                chunks.append(chunk.strip())
                start += max_chunk_chars - overlap

        # Attach metadata to each chunk
        for chunk_text in chunks:
            chunk = {
                "company": item.get("company"),
                "quarter": item.get("quarter"),
                "year": item.get("year"),
                "date": item.get("date"),
                "speaker": item.get("speaker"),
                "role": item.get("role"),
                "content": chunk_text
            }
            all_chunks.append(chunk)

    return all_chunks



#embedding
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
from tqdm import tqdm

# Load pre-trained open-source model (small and fast)
model = SentenceTransformer('all-MiniLM-L6-v2')

def embed_chunks(chunks):
    texts = [chunk['content'] for chunk in chunks]
    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)
    return embeddings.astype('float32')

def build_faiss_index(embedding_vectors):
    dim = embedding_vectors.shape[1]
    index = faiss.IndexFlatL2(dim)  # L2 distance index
    index.add(embedding_vectors)
    return index


import json
import faiss
from utils.load_data import load_transcript_data
from utils.validate_data import validate_transcripts
from utils.chunk_data import prepare_chunks
from utils.embed_data import embed_chunks, build_faiss_index

def save_json(data, path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def save_faiss_index(index, path):
    faiss.write_index(index, path)

def main():
    data = load_transcript_data()
    errors = validate_transcripts(data)
    if errors:
        print("‚ö†Ô∏è Validation errors:")
        for err in errors:
            print(err)
        return
    print("‚úÖ Data valid.")

    chunks = prepare_chunks(data)
    print(f"Chunks prepared: {len(chunks)}")

    embeddings = embed_chunks(chunks)
    print("‚úÖ Embeddings generated.")

    index = build_faiss_index(embeddings)
    print("‚úÖ FAISS index built.")

    save_json(chunks, 'Data_Ingestion/chunks_metadata.json')
    save_faiss_index(index, 'Data_Ingestion/faiss_index.bin')

    print("‚úÖ Saved chunk metadata and FAISS index.")

if __name__ == "__main__":
    main()


#search.py to load index + metadata and perform querries.

import json
import faiss
import numpy as np
from utils.embed_data import model

def load_faiss_index(path='Data_Ingestion/faiss_index.bin'):
    return faiss.read_index(path)

def load_chunks_metadata(path='Data_Ingestion/chunks_metadata.json'):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def search(query, chunks, index, top_k=5):
    q_emb = model.encode([query], convert_to_numpy=True).astype('float32')
    distances, indices = index.search(q_emb, top_k)
    results = [chunks[i] for i in indices[0]]
    return results

if __name__ == "__main__":
    index = load_faiss_index()
    chunks = load_chunks_metadata()

    query = input("Enter your query: ")
    results = search(query, chunks, index, top_k=5)

    print("\nTop Results:\n")
    for res in results:
        print(f"{res['date']} - {res['speaker']} ({res['role']}) at {res['company']}:")
        print(res['content'])
        print("-" * 80)


# app.py
import streamlit as st
from sentence_transformers import SentenceTransformer
import faiss
import json
import textwrap

# Load model and index
model = SentenceTransformer("all-MiniLM-L6-v2")
index = faiss.read_index("Data_Ingestion/faiss_index.bin")

# Load metadata
with open("Data_Ingestion/chunks_metadata.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

st.title("üìä Financial Transcript Search")
st.subheader("Semantic search over earnings call transcripts")

query = st.text_input("Enter your financial query:")
top_k = st.slider("Number of results", min_value=1, max_value=10, value=5)

if st.button("Search") and query:
    q_emb = model.encode([query], convert_to_numpy=True).astype("float32")
    distances, indices = index.search(q_emb, top_k)

    for idx, i in enumerate(indices[0]):
        item = chunks[i]
        score = round(1 / (1 + distances[0][idx]), 4)
        st.markdown(f"""
        ### {item['company']} | {item['quarter']} {item['year']}  
        üóìÔ∏è {item['date']}  
        üßë {item['speaker']} ({item['role']})  
        üîç **Relevance Score**: {score}  
        > {textwrap.shorten(item['content'], width=500, placeholder="...")}
        ---
        """)


# rag_pipeline.py

import json
import faiss
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import textwrap

class RAGFinancialAssistant:
    def __init__(self,
                 faiss_index_path="Data_Ingestion/faiss_index.bin",
                 chunks_metadata_path="Data_Ingestion/chunks_metadata.json",
                 embedding_model_name="sentence-transformers/all-MiniLM-L6-v2",
                 generation_model_name="facebook/bart-large-cnn"):
        
        # Load FAISS index
        print("Loading FAISS index...")
        self.index = faiss.read_index(faiss_index_path)
        
        # Load chunks metadata
        print("Loading chunk metadata...")
        with open(chunks_metadata_path, "r", encoding="utf-8") as f:
            self.chunks = json.load(f)
        
        # Load embedding model
        print("Loading embedding model...")
        self.embedder = SentenceTransformer(embedding_model_name)
        
        # Load generation model pipeline
        print("Loading generation model...")
        self.generator = pipeline("summarization", model=generation_model_name)

    def search_chunks(self, query, top_k=5):
        # Embed the query
        q_emb = self.embedder.encode([query], convert_to_numpy=True).astype('float32')
        
        # Search in FAISS index
        distances, indices = self.index.search(q_emb, top_k)
        
        # Collect the relevant chunks with metadata
        results = []
        for idx in indices[0]:
            chunk = self.chunks[idx]
            results.append(chunk)
        return results, distances[0]

    def generate_answer(self, query, top_k=5):
        # Retrieve top relevant chunks
        chunks, distances = self.search_chunks(query, top_k=top_k)
        
        # Combine their content into one context string (you can tune this)
        context_text = " ".join([chunk['content'] for chunk in chunks])
        
        # Format input to generation model: you can prepend query to context if you want
        input_text = f"Question: {query}\nContext: {context_text}"
        
        # Generate summary/answer (tune max_length & min_length as needed)
        summary = self.generator(input_text, max_length=150, min_length=40, do_sample=False)
        
        return summary[0]['summary_text'], chunks, distances


if __name__ == "__main__":
    rag = RAGFinancialAssistant()
    
    user_query = input("Enter your financial query: ")
    answer, retrieved_chunks, scores = rag.generate_answer(user_query, top_k=5)
    
    print("\n=== Generated Answer ===")
    print(answer)
    
    print("\n=== Retrieved Chunks (metadata + truncated content) ===")
    for i, chunk in enumerate(retrieved_chunks):
        print(f"[{i+1}] {chunk['company']} | {chunk['date']} | Speaker: {chunk['speaker']} | Role: {chunk['role']}")
        print(textwrap.shorten(chunk['content'], width=300, placeholder="..."))
        print(f"Similarity score: {scores[i]:.4f}")
        print("-" * 80)




#Below is the LANGGRAPH based RAG agent.

#utils/retrieve.py
import faiss
import json
import numpy as np
from utils.embed_data import model  # Your SentenceTransformer model loading here

# Load once (make sure paths match your setup)
index = faiss.read_index("Data_Ingestion/faiss_index.bin")
with open("Data_Ingestion/chunks_metadata.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

def retrieve_chunks(query: str, top_k=5):
    # Embed query
    q_emb = model.encode([query], convert_to_numpy=True).astype('float32')

    # Search FAISS
    distances, indices = index.search(q_emb, top_k)

    results = []
    for i in indices[0]:
        item = chunks[i]
        results.append(item["content"])

    # Return concatenated chunk content (or list if you prefer)
    return "\n".join(results)


#utils/generate.py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

def generate_answer(query: str, retrieved_text: str):
    input_text = f"Question: {query}\nContext: {retrieved_text}\nAnswer:"
    inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)

    outputs = model.generate(
        **inputs,
        max_length=150,
        num_beams=5,
        early_stopping=True
    )
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer


#agent.py
from langgraph.graph import Graph
from langgraph.nodes import FunctionNode

from utils.retrieve import retrieve_chunks
from utils.generate import generate_answer

# Wrap functions as LangGraph nodes
retrieve_node = FunctionNode(func=retrieve_chunks, name="Retriever")
generate_node = FunctionNode(func=generate_answer, name="Generator")

# Build graph
graph = Graph(name="Financial RAG Agent")
graph.add_node(retrieve_node)
graph.add_node(generate_node)

# Connect retrieve output to generate input
graph.add_edge(retrieve_node, generate_node, output_key="retrieved_text", input_key="retrieved_text")

def run_agent(query: str):
    inputs = {"query": query}
    outputs = graph.run(inputs)
    # outputs will be dict with 'answer' key matching generate_answer return
    return outputs.get("answer", "No answer generated")


#main.py
import os
import json
import faiss
from utils.load_data import load_transcript_data
from utils.validate_data import validate_transcripts
from utils.chunk_data import prepare_chunks
from utils.embed_data import embed_chunks, build_faiss_index
from agent import run_agent  # your existing agent function to handle queries

CHUNKS_PATH = 'Data_Ingestion/chunks_metadata.json'
INDEX_PATH = 'Data_Ingestion/faiss_index.bin'

def save_json(data, path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def save_faiss_index(index, path):
    faiss.write_index(index, path)

def load_faiss_index(path):
    return faiss.read_index(path)

def load_json(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def prepare_data():
    print("Loading and validating data...")
    data = load_transcript_data()
    errors = validate_transcripts(data)
    if errors:
        print("‚ö†Ô∏è Validation errors:")
        for err in errors:
            print(err)
        return None, None
    print("‚úÖ Data valid.")

    print("Preparing chunks...")
    chunks = prepare_chunks(data)
    print(f"Chunks prepared: {len(chunks)}")

    print("Generating embeddings...")
    embeddings = embed_chunks(chunks)
    print("‚úÖ Embeddings generated.")

    print("Building FAISS index...")
    index = build_faiss_index(embeddings)
    print("‚úÖ FAISS index built.")

    print("Saving chunk metadata and FAISS index...")
    save_json(chunks, CHUNKS_PATH)
    save_faiss_index(index, INDEX_PATH)
    print("‚úÖ Save complete.")

    return chunks, index

def main():
    # Check if index and metadata exist to skip reprocessing
    if os.path.exists(CHUNKS_PATH) and os.path.exists(INDEX_PATH):
        print("Loading existing FAISS index and chunk metadata...")
        chunks = load_json(CHUNKS_PATH)
        index = load_faiss_index(INDEX_PATH)
    else:
        chunks, index = prepare_data()
        if chunks is None or index is None:
            print("Data preparation failed. Exiting.")
            return

    print("\nReady for queries. Type 'exit' to quit.")
    while True:
        query = input("\nEnter your financial question: ")
        if query.lower() in ['exit', 'quit']:
            print("Exiting. Goodbye!")
            break
        answer = run_agent(query, chunks=chunks, index=index)  # pass loaded data
        print("\nAnswer:")
        print(answer)

if __name__ == "__main__":
    main()

