📁 project_folder/
│
├── Data_Ingestion/
│   └── final_cleaned_ticker_data.json
│
├── main.py                  # To run everything
├── utils/
│   ├── __init__.py
│   ├── load_data.py         # Load data
│   ├── filter_data.py       # Filtering function
│   └── validate_data.py     # Validation script


# utils/load_data.py
import json
import os

def load_transcript_data(file_path='Data_Ingestion/final_cleaned_ticker_data.json'):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


# utils/filter_data.py
def filter_transcripts(data, company=None, year=None, quarter=None, keyword=None):
    results = []
    for item in data:
        if company and company.lower() not in item['company'].lower():
            continue
        if year and int(item['year']) != int(year):
            continue
        if quarter and item['quarter'].upper() != quarter.upper():
            continue
        if keyword and keyword.lower() not in item['content'].lower():
            continue
        results.append(item)
    return results


# utils/validate_data.py
def validate_transcripts(data):
    required_fields = {'company', 'file_name', 'quarter', 'year', 'date', 'speaker', 'role', 'content'}
    errors = []
    for idx, item in enumerate(data):
        missing = required_fields - item.keys()
        if missing:
            errors.append(f"Item {idx} is missing fields: {missing}")
        elif not all(item[field] for field in required_fields):
            errors.append(f"Item {idx} has empty fields")
    return errors

# main.py
import json
from utils.load_data import load_transcript_data
from utils.filter_data import filter_transcripts
from utils.validate_data import validate_transcripts
from utils.chunk_data import prepare_chunks

def save_chunked_data(chunks, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    print(f"✅ Chunked data saved to {file_path}")

# Step 1: Load
data = load_transcript_data()

# Step 2: Validate
errors = validate_transcripts(data)
if errors:
    print("⚠️ Data validation errors found:")
    for err in errors:
        print(err)
    exit(1)
else:
    print("✅ Data is valid.\n")

# Step 3: Filter
results = filter_transcripts(data, company='Apple', year=2025, quarter='Q1', keyword='sales')

# Step 4: Chunking
chunks = prepare_chunks(data)
print(f"✅ Prepared {len(chunks)} semantic chunks.")
print("Sample chunk:")
print(chunks[0])

# Step 4.1: Save chunked data
chunked_file_path = 'Data_Ingestion/final_cleaned_ticker_data_chunked.json'
save_chunked_data(chunks, chunked_file_path)

# Step 5: Show filtered results
for res in results:
    print(f"{res['date']} - {res['speaker']} ({res['role']}):")
    print(res['content'])
    print('-' * 80)


#chunking
import textwrap

def prepare_chunks(data, max_chunk_chars=1000, overlap=200):
    """
    Prepares semantic chunks with metadata.
    Long content is split into smaller overlapping chunks.

    Parameters:
    - data: list of transcript dicts (already per speaker)
    - max_chunk_chars: max characters per chunk (default: 1000)
    - overlap: number of overlapping chars between chunks (default: 200)

    Returns:
    - List of chunk dicts with metadata and short content
    """
    all_chunks = []

    for item in data:
        content = item.get("content", "")
        if not content.strip():
            continue

        # Break content into overlapping chunks
        chunks = []
        if len(content) <= max_chunk_chars:
            chunks = [content]
        else:
            start = 0
            while start < len(content):
                end = start + max_chunk_chars
                chunk = content[start:end]
                chunks.append(chunk.strip())
                start += max_chunk_chars - overlap

        # Attach metadata to each chunk
        for chunk_text in chunks:
            chunk = {
                "company": item.get("company"),
                "quarter": item.get("quarter"),
                "year": item.get("year"),
                "date": item.get("date"),
                "speaker": item.get("speaker"),
                "role": item.get("role"),
                "content": chunk_text
            }
            all_chunks.append(chunk)

    return all_chunks



#embedding
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
from tqdm import tqdm

# Load pre-trained open-source model (small and fast)
model = SentenceTransformer('all-MiniLM-L6-v2')

def embed_chunks(chunks):
    texts = [chunk['content'] for chunk in chunks]
    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)
    return embeddings.astype('float32')

def build_faiss_index(embedding_vectors):
    dim = embedding_vectors.shape[1]
    index = faiss.IndexFlatL2(dim)  # L2 distance index
    index.add(embedding_vectors)
    return index


import json
import faiss
from utils.load_data import load_transcript_data
from utils.validate_data import validate_transcripts
from utils.chunk_data import prepare_chunks
from utils.embed_data import embed_chunks, build_faiss_index

def save_json(data, path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def save_faiss_index(index, path):
    faiss.write_index(index, path)

def main():
    data = load_transcript_data()
    errors = validate_transcripts(data)
    if errors:
        print("⚠️ Validation errors:")
        for err in errors:
            print(err)
        return
    print("✅ Data valid.")

    chunks = prepare_chunks(data)
    print(f"Chunks prepared: {len(chunks)}")

    embeddings = embed_chunks(chunks)
    print("✅ Embeddings generated.")

    index = build_faiss_index(embeddings)
    print("✅ FAISS index built.")

    save_json(chunks, 'Data_Ingestion/chunks_metadata.json')
    save_faiss_index(index, 'Data_Ingestion/faiss_index.bin')

    print("✅ Saved chunk metadata and FAISS index.")

if __name__ == "__main__":
    main()


#search.py to load index + metadata and perform querries.

import json
import faiss
import numpy as np
from utils.embed_data import model

def load_faiss_index(path='Data_Ingestion/faiss_index.bin'):
    return faiss.read_index(path)

def load_chunks_metadata(path='Data_Ingestion/chunks_metadata.json'):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def search(query, chunks, index, top_k=5):
    q_emb = model.encode([query], convert_to_numpy=True).astype('float32')
    distances, indices = index.search(q_emb, top_k)
    results = [chunks[i] for i in indices[0]]
    return results

if __name__ == "__main__":
    index = load_faiss_index()
    chunks = load_chunks_metadata()

    query = input("Enter your query: ")
    results = search(query, chunks, index, top_k=5)

    print("\nTop Results:\n")
    for res in results:
        print(f"{res['date']} - {res['speaker']} ({res['role']}) at {res['company']}:")
        print(res['content'])
        print("-" * 80)


#Gradio app used to display result for sementically analysing the querry in the vector db.
import gradio as gr
import faiss
import json
import textwrap
from sentence_transformers import SentenceTransformer

# Load model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Load FAISS index
index = faiss.read_index("Data_Ingestion/faiss_index.bin")

# Load metadata
with open("Data_Ingestion/chunks_metadata.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

# Define the search function
def search_transcripts(query, top_k):
    q_emb = model.encode([query], convert_to_numpy=True).astype('float32')
    distances, indices = index.search(q_emb, top_k)

    results = []
    for idx, i in enumerate(indices[0]):
        item = chunks[i]
        score = round(1 / (1 + distances[0][idx]), 4)
        text = f"""**{item['company']} | {item['quarter']} {item['year']} | {item['date']}**  
🧑 {item['speaker']} ({item['role']})  
📄 **Score**: {score}  
{textwrap.shorten(item['content'], width=400, placeholder="...")}"""
        results.append(text)
    
    return "\n\n---\n\n".join(results)

# Create Gradio interface
iface = gr.Interface(
    fn=search_transcripts,
    inputs=[
        gr.Textbox(lines=2, placeholder="Enter financial question...", label="Your Query"),
        gr.Slider(minimum=1, maximum=10, step=1, value=5, label="Top K Results")
    ],
    outputs=gr.Markdown(label="Results"),
    title="🔍 Financial Transcript Search",
    description="Query a financial Q&A database with semantic search powered by FAISS + SBERT."
)

# Launch it
iface.launch(share=False)
