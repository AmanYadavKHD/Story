📁 project_folder/
│
├── Data_Ingestion/
│   └── final_cleaned_ticker_data.json
│
├── main.py                  # To run everything
├── utils/
│   ├── __init__.py
│   ├── load_data.py         # Load data
│   ├── filter_data.py       # Filtering function
│   └── validate_data.py     # Validation script


# utils/load_data.py
import json
import os

def load_transcript_data(file_path='Data_Ingestion/final_cleaned_ticker_data.json'):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


# utils/filter_data.py
def filter_transcripts(data, company=None, year=None, quarter=None, keyword=None):
    results = []
    for item in data:
        if company and company.lower() not in item['company'].lower():
            continue
        if year and int(item['year']) != int(year):
            continue
        if quarter and item['quarter'].upper() != quarter.upper():
            continue
        if keyword and keyword.lower() not in item['content'].lower():
            continue
        results.append(item)
    return results


# utils/validate_data.py
def validate_transcripts(data):
    required_fields = {'company', 'file_name', 'quarter', 'year', 'date', 'speaker', 'role', 'content'}
    errors = []
    for idx, item in enumerate(data):
        missing = required_fields - item.keys()
        if missing:
            errors.append(f"Item {idx} is missing fields: {missing}")
        elif not all(item[field] for field in required_fields):
            errors.append(f"Item {idx} has empty fields")
    return errors


# main.py
from utils.load_data import load_transcript_data
from utils.filter_data import filter_transcripts
from utils.validate_data import validate_transcripts
from utils.chunk_data import prepare_chunks
# Step 1: Load
data = load_transcript_data()

# Step 2: Validate
errors = validate_transcripts(data)
if errors:
    print("⚠️ Data validation errors found:")
    for err in errors:
        print(err)
    exit(1)
else:
    print("✅ Data is valid.\n")

# Step 3: Filter
results = filter_transcripts(data, company='Apple', year=2025, quarter='Q1', keyword='sales')

# Step 4: Chunking
chunks = prepare_chunks(data)
print(f"✅ Prepared {len(chunks)} semantic chunks.")
print(chunks[0])

# Step 5: Show results
for res in results:
    print(f"{res['date']} - {res['speaker']} ({res['role']}):")
    print(res['content'])
    print('-' * 80)

#chunking
import textwrap

def prepare_chunks(data, max_chunk_chars=1000, overlap=200):
    """
    Prepares semantic chunks with metadata.
    Long content is split into smaller overlapping chunks.

    Parameters:
    - data: list of transcript dicts (already per speaker)
    - max_chunk_chars: max characters per chunk (default: 1000)
    - overlap: number of overlapping chars between chunks (default: 200)

    Returns:
    - List of chunk dicts with metadata and short content
    """
    all_chunks = []

    for item in data:
        content = item.get("content", "")
        if not content.strip():
            continue

        # Break content into overlapping chunks
        chunks = []
        if len(content) <= max_chunk_chars:
            chunks = [content]
        else:
            start = 0
            while start < len(content):
                end = start + max_chunk_chars
                chunk = content[start:end]
                chunks.append(chunk.strip())
                start += max_chunk_chars - overlap

        # Attach metadata to each chunk
        for chunk_text in chunks:
            chunk = {
                "company": item.get("company"),
                "quarter": item.get("quarter"),
                "year": item.get("year"),
                "date": item.get("date"),
                "speaker": item.get("speaker"),
                "role": item.get("role"),
                "content": chunk_text
            }
            all_chunks.append(chunk)

    return all_chunks
